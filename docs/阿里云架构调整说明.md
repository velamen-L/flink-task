# 阿里云架构调整说明

## 🎯 概述

本文档详细说明了基于阿里云生态的Flink实时计算架构调整，包括Catalog集成、VVP平台适配、监控体系构建等技术方案。

## 🏗️ 阿里云集成架构

### 整体架构图
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   业务系统      │    │   阿里云Kafka    │    │   阿里云VVP     │
│                │    │                │    │                │
│ ┌─────────────┐ │    │ ┌─────────────┐ │    │ ┌─────────────┐ │
│ │ 业务事件    │ │───▶│ │ 统一Topic   │ │───▶│ │ Flink作业   │ │
│ │ 生成        │ │    │ │ wrongbook-  │ │    │ │ 集群        │ │
│ └─────────────┘ │    │ │ events      │ │    │ └─────────────┘ │
└─────────────────┘    └─────────────────┘    │                │
                                              │ ┌─────────────┐ │
                                              │ │ Catalog     │ │
                                              │ │ 表管理      │ │
                                              │ └─────────────┘ │
                                              │                │
                                              │ ┌─────────────┐ │
                                              │ │ 监控告警    │ │
                                              │ │ 体系        │ │
                                              │ └─────────────┘ │
                                              └─────────────────┘
                                                       │
                                                       ▼
                                              ┌─────────────────┐
                                              │   数据输出      │
                                              │                │
                                              │ ┌─────────────┐ │
                                              │ │ 阿里云RDS   │ │
                                              │ │ MySQL       │ │
                                              │ └─────────────┘ │
                                              │                │
                                              │ ┌─────────────┐ │
                                              │ │ 阿里云SLS   │ │
                                              │ │ 日志服务    │ │
                                              │ └─────────────┘ │
                                              └─────────────────┘
```

### 核心组件

#### 1. 阿里云Kafka
- **功能**: 统一的事件存储和分发
- **优势**: 高可用、高性能、易扩展
- **配置**: 支持SASL认证、SSL加密

#### 2. 阿里云VVP
- **功能**: 实时计算平台
- **优势**: 托管服务、自动扩缩容
- **集成**: 支持Catalog、监控、告警

#### 3. 阿里云Catalog
- **功能**: 统一表结构管理
- **优势**: 简化配置、版本控制
- **支持**: 多种数据源和格式

## 🔧 Catalog集成方案

### 1. Catalog配置

#### 配置文件
```yaml
# catalog-config.yaml
catalogs:
  - name: aliyun_catalog
    type: hive
    default-database: dwd
    hive-conf-dir: /opt/flink/conf
    hive-version: 3.1.2
    configuration:
      hive.metastore.uris: thrift://metastore.aliyun.com:9083
      hive.metastore.warehouse.dir: oss://bucket-name/warehouse
      fs.oss.endpoint: oss-cn-hangzhou.aliyuncs.com
      fs.oss.accessKeyId: ${ACCESS_KEY_ID}
      fs.oss.accessKeySecret: ${ACCESS_KEY_SECRET}
```

#### 表结构定义
```sql
-- 源表定义
CREATE TABLE aliyun_catalog.dwd.wrongbook_events (
    eventId STRING,
    domain STRING,
    type STRING,
    timestamp BIGINT,
    payload STRING,
    proc_time AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'wrongbook-events',
    'properties.bootstrap.servers' = 'kafka.aliyun.com:9092',
    'properties.group.id' = 'wrongbook-wide-table-group',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);

-- 结果表定义
CREATE TABLE aliyun_catalog.dwd.dwd_wrong_record_wide_delta (
    id BIGINT,
    wrong_id STRING,
    user_id STRING,
    subject STRING,
    subject_name STRING,
    question_id STRING,
    question STRING,
    pattern_id STRING,
    pattern_name STRING,
    teach_type_id STRING,
    teach_type_name STRING,
    collect_time TIMESTAMP(3),
    fix_id STRING,
    fix_time TIMESTAMP(3),
    fix_result BIGINT,
    fix_result_desc STRING,
    event_id STRING,
    event_type STRING,
    process_time TIMESTAMP(3)
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://rds.aliyun.com:3306/dwd',
    'table-name' = 'dwd_wrong_record_wide_delta',
    'username' = '${MYSQL_USERNAME}',
    'password' = '${MYSQL_PASSWORD}',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);
```

### 2. SQL作业适配

#### 基于Catalog的SQL
```sql
-- 使用Catalog中的表
USE CATALOG aliyun_catalog;
USE DATABASE dwd;

-- 创建处理视图
CREATE TEMPORARY VIEW wrongbook_add_events AS
SELECT 
    eventId,
    domain,
    type,
    timestamp,
    TO_TIMESTAMP_LTZ(timestamp, 3) as event_time,
    PROCTIME() as proc_time,
    JSON_VALUE(payload, '$.userId') as userid,
    JSON_VALUE(payload, '$.questionId') as questionid,
    JSON_VALUE(payload, '$.subject') as subject,
    JSON_VALUE(payload, '$.subjectName') as subject_name,
    JSON_VALUE(payload, '$.patternId') as pattern_id,
    JSON_VALUE(payload, '$.chapterId') as chapter_id,
    JSON_VALUE(payload, '$.chapterName') as chapter_name,
    JSON_VALUE(payload, '$.wrongTime') as wrong_time,
    JSON_VALUE(payload, '$.createTime') as create_time
FROM wrongbook_events 
WHERE domain = 'wrongbook' 
  AND type = 'wrongbook_add';

-- 处理错题添加事件
INSERT INTO dwd_wrong_record_wide_delta
SELECT 
    CAST(HASH_CODE(COALESCE(eventId)) AS BIGINT) as id,
    eventId as wrong_id,
    userid as user_id,
    subject,
    subject_name,
    questionid as question_id,
    '' as question,
    pattern_id,
    '' as pattern_name,
    chapter_id,
    chapter_name,
    TO_TIMESTAMP_LTZ(CAST(wrong_time AS BIGINT), 3) as collect_time,
    '' as fix_id,
    CAST(NULL AS TIMESTAMP(3)) as fix_time,
    0 as fix_result,
    '未订正' as fix_result_desc,
    eventId as event_id,
    type as event_type,
    proc_time as process_time
FROM wrongbook_add_events;
```

## 🚀 VVP平台适配

### 1. 作业配置

#### 作业参数配置
```yaml
# vvp-job-config.yaml
job:
  name: "错题本实时宽表"
  type: "SQL"
  version: "1.0"
  
flink:
  parallelism: 2
  checkpoint-interval: 10000
  checkpoint-mode: "EXACTLY_ONCE"
  state-backend: "rocksdb"
  state-checkpoints-dir: "oss://bucket-name/checkpoints"
  
resources:
  jobmanager:
    memory: "2GB"
    cpu: 1
  taskmanager:
    memory: "4GB"
    cpu: 2
    replicas: 2
    
catalog:
  name: "aliyun_catalog"
  type: "hive"
  configuration:
    hive.metastore.uris: "thrift://metastore.aliyun.com:9083"
```

#### 环境变量配置
```bash
# 环境变量
export ACCESS_KEY_ID="your_access_key_id"
export ACCESS_KEY_SECRET="your_access_key_secret"
export MYSQL_USERNAME="your_mysql_username"
export MYSQL_PASSWORD="your_mysql_password"
export KAFKA_USERNAME="your_kafka_username"
export KAFKA_PASSWORD="your_kafka_password"
```

### 2. 部署流程

#### 自动部署脚本
```bash
#!/bin/bash
# deploy-to-vvp.sh

# 1. 上传SQL文件
echo "上传SQL文件到VVP..."
vvp-cli sql upload \
  --file wrongbook_wide_table.sql \
  --project wrongbook-project \
  --name "错题本实时宽表"

# 2. 创建作业
echo "创建Flink作业..."
vvp-cli job create \
  --name "错题本实时宽表" \
  --sql-file wrongbook_wide_table.sql \
  --config vvp-job-config.yaml \
  --project wrongbook-project

# 3. 启动作业
echo "启动作业..."
vvp-cli job start \
  --name "错题本实时宽表" \
  --project wrongbook-project

echo "部署完成！"
```

#### 监控配置
```yaml
# monitoring-config.yaml
monitoring:
  metrics:
    - name: "events_processed_per_second"
      type: "counter"
      description: "每秒处理事件数"
      
    - name: "processing_latency"
      type: "gauge"
      description: "处理延迟"
      
    - name: "error_rate"
      type: "gauge"
      description: "错误率"
      
  alerts:
    - name: "high_latency"
      condition: "processing_latency > 30s"
      severity: "critical"
      notification:
        type: "webhook"
        url: "https://webhook.aliyun.com/alert"
        
    - name: "high_error_rate"
      condition: "error_rate > 1%"
      severity: "warning"
      notification:
        type: "email"
        recipients: ["admin@company.com"]
```

## 📊 监控体系构建

### 1. 阿里云SLS集成

#### 日志配置
```yaml
# log-config.yaml
logging:
  level: "INFO"
  pattern: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
  
  appenders:
    - name: "console"
      type: "console"
      
    - name: "file"
      type: "file"
      file: "/var/log/flink/wrongbook-wide-table.log"
      max-size: "100MB"
      max-files: 10
      
    - name: "sls"
      type: "aliyun-sls"
      endpoint: "cn-hangzhou.log.aliyuncs.com"
      project: "wrongbook-project"
      logstore: "flink-logs"
      access-key-id: "${ACCESS_KEY_ID}"
      access-key-secret: "${ACCESS_KEY_SECRET}"
```

#### 日志收集
```java
// 日志记录示例
public class WrongbookWideTableApp {
    private static final Logger logger = LoggerFactory.getLogger(WrongbookWideTableApp.class);
    
    public static void main(String[] args) throws Exception {
        logger.info("启动错题本实时宽表作业");
        
        try {
            // 作业逻辑
            logger.info("作业启动成功");
        } catch (Exception e) {
            logger.error("作业启动失败", e);
            throw e;
        }
    }
}
```

### 2. 阿里云ARMS集成

#### 应用监控配置
```yaml
# arms-config.yaml
arms:
  application:
    name: "wrongbook-wide-table"
    version: "1.0.0"
    
  tracing:
    enabled: true
    sample-rate: 0.1
    
  metrics:
    enabled: true
    interval: 60s
    
  alerts:
    - name: "job_failure"
      condition: "job_status == 'FAILED'"
      severity: "critical"
      
    - name: "high_memory_usage"
      condition: "memory_usage > 80%"
      severity: "warning"
```

#### 性能监控
```java
// 性能监控示例
public class PerformanceMonitor {
    private static final Counter eventsProcessed = Counter.builder("events_processed_total")
        .description("Total number of events processed")
        .register();
        
    private static final Gauge processingLatency = Gauge.builder("processing_latency_seconds")
        .description("Event processing latency")
        .register();
        
    public static void recordEventProcessed() {
        eventsProcessed.increment();
    }
    
    public static void recordLatency(double latency) {
        processingLatency.set(latency);
    }
}
```

### 3. 阿里云云监控集成

#### 监控大盘配置
```json
{
  "dashboard": {
    "name": "错题本实时宽表监控",
    "description": "实时监控错题本宽表处理情况",
    "panels": [
      {
        "title": "事件处理速率",
        "type": "line",
        "query": "events_processed_per_second",
        "yAxis": {
          "min": 0,
          "max": "auto"
        }
      },
      {
        "title": "处理延迟",
        "type": "line",
        "query": "processing_latency_seconds",
        "yAxis": {
          "min": 0,
          "max": 60
        }
      },
      {
        "title": "错误率",
        "type": "line",
        "query": "error_rate",
        "yAxis": {
          "min": 0,
          "max": 0.1
        }
      },
      {
        "title": "内存使用率",
        "type": "gauge",
        "query": "memory_usage_percent",
        "yAxis": {
          "min": 0,
          "max": 100
        }
      }
    ]
  }
}
```

## 🔧 安全配置

### 1. 访问控制

#### RAM权限配置
```json
{
  "Version": "1",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kafka:DescribeTopic",
        "kafka:ConsumeMessage",
        "rds:DescribeDBInstances",
        "rds:DescribeDatabases",
        "rds:DescribeTables",
        "rds:ModifyDatabase",
        "oss:GetObject",
        "oss:PutObject",
        "oss:ListObjects",
        "log:GetLogs",
        "log:PutLogs"
      ],
      "Resource": [
        "acs:kafka:*:*:topic/wrongbook-events",
        "acs:rds:*:*:dbinstance/wrongbook-rds",
        "acs:oss:*:*:bucket/wrongbook-bucket/*",
        "acs:log:*:*:project/wrongbook-project"
      ]
    }
  ]
}
```

#### 网络安全配置
```yaml
# network-config.yaml
network:
  vpc:
    id: "vpc-xxx"
    cidr: "10.0.0.0/16"
    
  security-groups:
    - name: "flink-sg"
      rules:
        - protocol: "tcp"
          port: "8081"
          source: "10.0.0.0/16"
          description: "Flink Web UI"
        - protocol: "tcp"
          port: "9092"
          source: "10.0.0.0/16"
          description: "Kafka"
        - protocol: "tcp"
          port: "3306"
          source: "10.0.0.0/16"
          description: "MySQL"
```

### 2. 数据加密

#### 传输加密
```yaml
# encryption-config.yaml
encryption:
  kafka:
    ssl:
      enabled: true
      truststore: "/opt/flink/conf/kafka.truststore.jks"
      keystore: "/opt/flink/conf/kafka.keystore.jks"
      
  mysql:
    ssl:
      enabled: true
      verify-server-cert: true
      
  oss:
    encryption:
      algorithm: "AES256"
      key-id: "alias/wrongbook-key"
```

#### 存储加密
```sql
-- 数据库表加密
CREATE TABLE dwd_wrong_record_wide_delta (
    -- 字段定义
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://rds.aliyun.com:3306/dwd?useSSL=true&requireSSL=true',
    'table-name' = 'dwd_wrong_record_wide_delta',
    'username' = '${MYSQL_USERNAME}',
    'password' = '${MYSQL_PASSWORD}'
) ENCRYPTED WITH (
    'algorithm' = 'AES256',
    'key-id' = 'alias/wrongbook-key'
);
```

## 🚀 性能优化

### 1. 资源配置优化

#### 内存配置
```yaml
# memory-config.yaml
memory:
  jobmanager:
    heap-size: "2GB"
    off-heap-size: "1GB"
    
  taskmanager:
    heap-size: "4GB"
    off-heap-size: "2GB"
    managed-memory-size: "2GB"
    network-memory-size: "1GB"
    
  state:
    backend: "rocksdb"
    checkpoint-dir: "oss://bucket-name/checkpoints"
    state-ttl: "1d"
```

#### 并行度配置
```yaml
# parallelism-config.yaml
parallelism:
  source: 3
  processing: 6
  sink: 3
  
  scaling:
    enabled: true
    min-parallelism: 2
    max-parallelism: 10
    scale-up-threshold: 0.8
    scale-down-threshold: 0.3
```

### 2. 网络优化

#### 网络配置
```yaml
# network-optimization.yaml
network:
  buffer-timeout: "100ms"
  buffer-size: "32KB"
  
  backpressure:
    enabled: true
    sampling-interval: "100ms"
    
  checkpoint:
    alignment-timeout: "10s"
    unaligned: true
```

### 3. 存储优化

#### 状态后端优化
```yaml
# state-backend-config.yaml
state:
  backend: "rocksdb"
  
  rocksdb:
    state.backend.rocksdb.memory.managed: true
    state.backend.rocksdb.memory.fixed-per-slot: "100MB"
    state.backend.rocksdb.memory.write-buffer-ratio: 0.4
    state.backend.rocksdb.memory.high-prio-pool-ratio: 0.1
    
  checkpoint:
    interval: "10s"
    timeout: "5min"
    min-pause: "2s"
    max-concurrent: 1
```

## 📈 成本优化

### 1. 资源成本优化

#### 实例规格选择
```yaml
# instance-config.yaml
instances:
  jobmanager:
    instance-type: "ecs.g6.large"
    spot-instance: false
    
  taskmanager:
    instance-type: "ecs.g6.xlarge"
    spot-instance: true
    max-price: "0.5"
    
  autoscaling:
    enabled: true
    min-instances: 2
    max-instances: 10
    scale-up-cooldown: "5min"
    scale-down-cooldown: "10min"
```

#### 存储成本优化
```yaml
# storage-cost-optimization.yaml
storage:
  checkpoint:
    retention: "7d"
    compression: "lz4"
    
  state:
    ttl: "1d"
    cleanup-mode: "full"
    
  log:
    retention: "30d"
    compression: "gzip"
```

### 2. 网络成本优化

#### 数据传输优化
```yaml
# network-cost-optimization.yaml
network:
  compression:
    enabled: true
    algorithm: "lz4"
    
  batching:
    enabled: true
    max-batch-size: "1MB"
    max-batch-time: "100ms"
    
  routing:
    prefer-same-zone: true
    use-private-network: true
```

## 🎉 总结

通过阿里云架构调整，我们实现了：

1. **Catalog集成**: 统一表结构管理，简化配置
2. **VVP平台适配**: 托管服务，自动扩缩容
3. **监控体系**: 完善的监控和告警机制
4. **安全配置**: 多层次的安全防护
5. **性能优化**: 针对性的性能调优
6. **成本优化**: 合理的资源配置和成本控制

这套阿里云集成方案为错题本实时宽表提供了完整的云原生解决方案，支持业务的快速发展和技术的持续演进。
