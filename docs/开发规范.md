# Flink实时计算开发规范

## 🎯 概述

本文档定义了基于AI编程脚手架和动态路由架构的Flink实时计算开发规范，确保代码质量、架构一致性和开发效率。

## 📁 项目结构规范

### 目录结构
```
flink-task/
├── src/main/java/com/flink/realtime/
│   ├── business/                           # 业务应用目录 ⭐
│   │   └── {Domain}WideTableApp.java      # 业务应用类
│   ├── bean/                               # 实体类
│   │   ├── BusinessEvent.java             # 业务事件实体
│   │   └── ProcessedEvent.java            # 处理后事件实体
│   ├── config/                             # 配置类
│   │   └── RoutingConfig.java             # 路由配置
│   ├── common/                             # 通用工具
│   │   └── AliyunFlinkUtils.java          # 阿里云Flink工具
│   ├── function/                           # 函数类
│   │   ├── BusinessEventDeserializationSchema.java
│   │   ├── DynamicRoutingProcessFunction.java
│   │   └── OutputRoutingProcessFunction.java
│   ├── processor/                          # 处理器
│   │   └── impl/                          # 处理器实现
│   ├── sink/                               # 输出类
│   │   └── AliyunMySQLSinkFunction.java   # MySQL输出
│   ├── source/                             # 数据源
│   │   ├── AliyunKafkaSourceBuilder.java  # Kafka源构建器
│   │   └── DynamicRoutingConfigSource.java # 动态路由配置源
│   └── util/                               # 工具类
│       └── ConfigUtils.java               # 配置工具
├── sql/                                    # SQL文件
├── scripts/                                # 脚本文件
│   ├── ai-job-generator.py               # AI作业生成器
│   ├── test-ai-generator.py              # 测试脚本
│   ├── examples/                          # 配置示例
│   └── templates/                         # 模板文件
└── docs/                                   # 文档
```

### 命名规范

#### 1. 包命名
- **业务应用**: `com.flink.realtime.business`
- **实体类**: `com.flink.realtime.bean`
- **配置类**: `com.flink.realtime.config`
- **函数类**: `com.flink.realtime.function`
- **处理器**: `com.flink.realtime.processor`
- **数据源**: `com.flink.realtime.source`
- **输出类**: `com.flink.realtime.sink`
- **工具类**: `com.flink.realtime.util`

#### 2. 类命名
- **业务应用**: `{Domain}WideTableApp` (如: `WrongbookWideTableApp`)
- **实体类**: 以实体名命名 (如: `BusinessEvent`, `ProcessedEvent`)
- **函数类**: 以功能+Function命名 (如: `DynamicRoutingProcessFunction`)
- **处理器**: 以业务+Processor命名 (如: `WrongbookAddProcessor`)
- **工具类**: 以功能+Utils命名 (如: `ConfigUtils`)

#### 3. 方法命名
- **主要方法**: 使用动词+名词 (如: `processEvent`, `buildKafkaSource`)
- **配置方法**: 使用get+配置名 (如: `getInsertSQL`, `getParameterSetter`)
- **工具方法**: 使用动词+功能 (如: `extractFromPayload`, `getSubjectName`)

## 🔧 代码开发规范

### 1. 业务应用开发

#### 基本结构
```java
package com.flink.realtime.business;

import com.flink.realtime.bean.BusinessEvent;
import com.flink.realtime.bean.ProcessedEvent;
import com.flink.realtime.config.RoutingConfig;
// ... 其他导入

/**
 * {业务名称}实时宽表 - AI生成
 * 业务域: {domain}
 * 生成时间: {timestamp}
 * 
 * 支持的事件类型:
 * - {event_type}: {description}
 * 
 * @author AI代码生成器
 */
public class {Domain}WideTableApp {
    
    private static final Logger logger = LoggerFactory.getLogger({Domain}WideTableApp.class);
    
    public static void main(String[] args) throws Exception {
        // 1. 创建执行环境
        // 2. 创建事件源
        // 3. 创建动态路由配置源
        // 4. 动态路由处理
        // 5. 输出路由
        // 6. 输出到宽表
        // 7. 执行作业
    }
    
    // 私有方法定义
}
```

#### 必需步骤
1. **执行环境创建**: 使用`AliyunFlinkUtils.getAliyunStreamExecutionEnvironment()`
2. **事件源创建**: 使用`AliyunKafkaSourceBuilder.buildAliyunKafkaSource()`
3. **动态路由配置**: 使用`DynamicRoutingConfigSource`
4. **事件处理**: 使用`DynamicRoutingProcessFunction`
5. **输出路由**: 使用`OutputRoutingProcessFunction`
6. **数据输出**: 使用`AliyunMySQLSinkFunction`

### 2. 配置文件规范

#### 配置文件结构
```json
{
  "domain": "业务域名",
  "job_name": "作业名称",
  "description": "作业描述",
  "source_table": {
    "name": "源表名",
    "type": "kafka",
    "topic": "topic名称",
    "format": "json"
  },
  "result_table": {
    "name": "结果表名",
    "type": "mysql",
    "database": "数据库名",
    "table": "表名"
  },
  "event_types": [
    {
      "type": "事件类型",
      "description": "事件描述",
      "payload_fields": [
        {
          "name": "字段名",
          "type": "字段类型",
          "required": true,
          "description": "字段描述"
        }
      ],
      "processing_logic": "处理逻辑",
      "output_fields": ["输出字段列表"]
    }
  ],
  "dimension_tables": [
    {
      "name": "维表名",
      "type": "mysql",
      "key_field": "主键字段",
      "query_sql": "查询SQL",
      "refresh_interval": "刷新间隔",
      "description": "维表描述"
    }
  ],
  "routing_config": {
    "enable_dynamic_routing": true,
    "config_source": "mysql",
    "config_table": "配置表名",
    "refresh_interval": "30s",
    "enable_hot_deployment": true,
    "enable_version_control": true
  }
}
```

### 3. 事件处理规范

#### 事件格式
```json
{
  "domain": "业务域",
  "type": "事件类型",
  "timestamp": 1640995200000,
  "eventId": "事件ID",
  "payload": {
    "字段1": "值1",
    "字段2": "值2"
  }
}
```

#### 处理器开发
```java
public class {Domain}{EventType}Processor implements EventProcessor {
    
    @Override
    public Object process(BusinessEvent event) throws Exception {
        // 1. 验证事件
        validateEvent(event);
        
        // 2. 提取数据
        Map<String, Object> data = extractData(event);
        
        // 3. 业务处理
        processBusinessLogic(data);
        
        // 4. 返回结果
        return data;
    }
    
    private void validateEvent(BusinessEvent event) {
        // 事件验证逻辑
    }
    
    private Map<String, Object> extractData(BusinessEvent event) {
        // 数据提取逻辑
    }
    
    private void processBusinessLogic(Map<String, Object> data) {
        // 业务处理逻辑
    }
}
```

## 🚀 开发工作流

### 1. 新业务开发流程

#### 步骤1: 定义配置
```bash
# 复制配置模板
cp scripts/examples/wrongbook-wide-table-config.json scripts/examples/{domain}-wide-table-config.json

# 修改配置文件
# - 更新domain
# - 定义event_types
# - 配置dimension_tables
# - 设置routing_config
```

#### 步骤2: 生成代码
```bash
# 使用AI生成器
python scripts/ai-job-generator.py scripts/examples/{domain}-wide-table-config.json

# 生成的Java文件位置
src/main/java/com/flink/realtime/business/{Domain}WideTableApp.java
```

#### 步骤3: 开发处理器
```bash
# 创建处理器类
src/main/java/com/flink/realtime/processor/impl/{Domain}{EventType}Processor.java
```

#### 步骤4: 测试验证
```bash
# 运行测试
python scripts/test-ai-generator.py

# 本地测试
mvn test
```

#### 步骤5: 部署运行
```bash
# 编译打包
mvn clean package -DskipTests

# 提交作业
$FLINK_HOME/bin/flink run \
  --class com.flink.realtime.business.{Domain}WideTableApp \
  target/flink-task-1.0-SNAPSHOT.jar {domain}
```

### 2. 现有业务维护流程

#### 添加新事件类型
1. **更新配置文件**: 在`event_types`中添加新事件类型
2. **开发处理器**: 创建对应的处理器类
3. **热部署**: 通过配置源更新路由配置
4. **监控验证**: 检查新事件类型的处理情况

#### 修改业务逻辑
1. **更新处理器**: 修改对应的处理器类
2. **重新部署**: 重新编译和部署作业
3. **验证功能**: 测试修改后的功能

## 📊 代码质量规范

### 1. 代码风格

#### 注释规范
- **类注释**: 包含功能描述、作者、时间
- **方法注释**: 包含参数说明、返回值、异常
- **关键逻辑注释**: 复杂业务逻辑必须有注释

#### 异常处理
```java
try {
    // 业务逻辑
} catch (Exception e) {
    logger.error("处理事件失败: {}", event, e);
    // 错误处理逻辑
    out.collect(event); // 继续处理，避免数据丢失
}
```

#### 日志规范
```java
// 使用占位符，避免字符串拼接
logger.info("启动{}宽表作业，业务域: {}", jobName, domain);
logger.error("处理事件失败: {}", event, e);
logger.debug("更新维表数据: {}", key);
```

### 2. 性能优化

#### 状态管理
- 使用`BroadcastState`进行配置管理
- 合理设置状态TTL，避免内存泄漏
- 使用`MapState`进行维表缓存

#### 并行度配置
- 根据数据量合理设置并行度
- 使用`ConfigUtils.getInt("flink.parallelism", 2)`
- 监控作业性能，动态调整

#### 批处理优化
- 使用批量输出减少网络开销
- 合理设置批大小：`AliyunMySQLSinkFunction<>(sql, setter, 100)`
- 监控批处理延迟

### 3. 监控规范

#### 关键指标
- **事件处理速率**: 每秒处理事件数
- **处理延迟**: 事件从接收到处理的时间
- **错误率**: 处理失败的事件比例
- **状态大小**: 内存中状态数据大小

#### 告警规则
- **处理延迟 > 30秒**: 立即告警
- **错误率 > 1%**: 告警并检查
- **状态大小异常**: 检查内存使用

## 🔄 版本控制规范

### 1. 分支管理
- **main**: 主分支，稳定版本
- **develop**: 开发分支，集成测试
- **feature/xxx**: 功能分支，新功能开发
- **hotfix/xxx**: 热修复分支，紧急修复

### 2. 提交规范
```
feat: 添加新功能
fix: 修复bug
docs: 更新文档
style: 代码格式调整
refactor: 代码重构
test: 添加测试
chore: 构建过程或辅助工具的变动
```

### 3. 版本号规范
- **主版本号**: 不兼容的API修改
- **次版本号**: 向下兼容的功能性新增
- **修订号**: 向下兼容的问题修正

## 🎉 总结

本开发规范确保了：

1. **架构一致性**: 所有业务应用遵循相同的架构模式
2. **代码质量**: 统一的代码风格和最佳实践
3. **开发效率**: 标准化的开发流程和工具
4. **维护性**: 清晰的文档和版本控制
5. **可扩展性**: 支持新业务快速接入

遵循本规范，可以确保项目的长期稳定性和可维护性。
