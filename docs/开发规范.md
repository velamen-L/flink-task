# Flink作业开发规范

## 1. 概述

本文档规定了基于本脚手架开发Flink实时计算作业的标准规范，包括代码规范、配置规范、测试规范等，以确保代码质量和系统稳定性。

## 2. 项目结构规范

### 2.1 标准项目结构

```
src/main/java/com/flink/realtime/
├── app/                    # 应用程序入口（必须）
│   ├── [Domain]DataStreamApp.java   # DataStream API应用
│   └── [Domain]SqlApp.java          # Flink SQL应用
├── bean/                   # 数据模型（必须）
│   ├── [Domain]Event.java  # 业务事件模型
│   └── [Domain]Result.java # 结果数据模型
├── processor/              # 事件处理器（DataStream模式必须）
│   └── impl/
│       └── [Domain]EventProcessor.java
├── source/                 # 自定义数据源（可选）
├── sink/                   # 自定义数据输出（可选）
├── function/               # UDF和序列化（可选）
└── config/                 # 配置类（可选）
```

### 2.2 命名规范

| 类型 | 命名规范 | 示例 |
|------|----------|------|
| **应用类** | `[Domain][Mode]App` | `UserDataStreamApp`、`OrderSqlApp` |
| **事件模型** | `[Domain]Event` | `UserEvent`、`OrderEvent` |
| **处理器** | `[Domain]EventProcessor` | `UserEventProcessor` |
| **配置文件** | `[domain]-[env].properties` | `user-dev.properties` |

## 3. 代码开发规范

### 3.1 DataStream API开发规范

#### 3.1.1 基本结构模板

```java
public class UserDataStreamApp {
    private static final Logger logger = LoggerFactory.getLogger(UserDataStreamApp.class);
    
    public static void main(String[] args) throws Exception {
        // 1. 创建执行环境
        StreamExecutionEnvironment env = FlinkUtils.getStreamExecutionEnvironment(1);
        
        // 2. 创建数据源
        DataStreamSource<BusinessEvent> eventStream = createEventSource(env);
        
        // 3. 创建维表广播流（如需要）
        BroadcastStream<Map<String, Object>> dimStream = createDimSource(env);
        
        // 4. 事件处理
        SingleOutputStreamOperator<ResultType> processedStream = processEvents(eventStream, dimStream);
        
        // 5. 输出结果
        outputResults(processedStream);
        
        // 6. 执行任务
        env.execute("User Event Processing Job");
    }
}
```

#### 3.1.2 必须遵循的规范

1. **环境配置**：必须使用`FlinkUtils.getStreamExecutionEnvironment()`
2. **异常处理**：所有可能的异常都必须被捕获和处理
3. **日志记录**：关键节点必须记录日志
4. **资源释放**：确保资源正确释放

### 3.2 Flink SQL开发规范

#### 3.2.1 基本结构模板

```java
public class UserSqlApp {
    private static final Logger logger = LoggerFactory.getLogger(UserSqlApp.class);
    
    public static void main(String[] args) throws Exception {
        // 1. 创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        
        // 2. 创建源表
        createSourceTables(tableEnv);
        
        // 3. 创建维表
        createDimTables(tableEnv);
        
        // 4. 创建结果表
        createSinkTables(tableEnv);
        
        // 5. 执行业务SQL
        executeBusinessSQL(tableEnv);
    }
    
    private static void createSourceTables(StreamTableEnvironment tableEnv) {
        // DDL语句创建源表
    }
}
```

#### 3.2.2 SQL编写规范

1. **表命名**：使用下划线分隔，如`user_event_source`
2. **字段命名**：使用下划线分隔，如`user_id`、`create_time`
3. **SQL格式化**：使用适当的缩进和换行
4. **注释**：复杂SQL必须添加注释说明

### 3.3 事件处理器开发规范

#### 3.3.1 接口实现

```java
public class UserEventProcessor implements EventProcessor {
    private static final Logger logger = LoggerFactory.getLogger(UserEventProcessor.class);
    
    @Override
    public Object process(BusinessEvent event) throws Exception {
        // 1. 参数验证
        validateEvent(event);
        
        // 2. 根据事件类型处理
        switch (event.getType()) {
            case "user_login":
                return processUserLogin(event.getPayload());
            case "user_purchase":
                return processUserPurchase(event.getPayload());
            default:
                throw new UnsupportedOperationException("不支持的事件类型: " + event.getType());
        }
    }
    
    @Override
    public String getSupportedEventType() {
        return "user_*";
    }
    
    private void validateEvent(BusinessEvent event) {
        if (event == null || event.getPayload() == null) {
            throw new IllegalArgumentException("事件或载荷数据不能为空");
        }
    }
}
```

#### 3.3.2 处理器规范

1. **异常处理**：必须处理所有可能的异常情况
2. **参数验证**：必须验证输入参数的有效性
3. **日志记录**：记录关键处理步骤和异常信息
4. **性能考虑**：避免在处理器中进行重IO操作

## 4. 配置管理规范

### 4.1 配置文件结构

```properties
# 应用基本信息
app.name=用户事件处理作业
app.version=1.0.0
app.domain=user

# Kafka配置
kafka.bootstrap.servers=${KAFKA_SERVERS:localhost:9092}
kafka.input.topic=${KAFKA_INPUT_TOPIC:user-events}
kafka.group.id=${KAFKA_GROUP_ID:user-event-processor}

# MySQL配置
mysql.url=${MYSQL_URL:jdbc:mysql://localhost:3306/user_db}
mysql.username=${MYSQL_USERNAME:root}
mysql.password=${MYSQL_PASSWORD:root}

# Flink配置
flink.parallelism=${FLINK_PARALLELISM:1}
flink.checkpoint.interval=${CHECKPOINT_INTERVAL:5000}
```

### 4.2 配置规范

1. **环境变量**：支持环境变量覆盖，格式`${ENV_VAR:default_value}`
2. **敏感信息**：密码等敏感信息必须通过环境变量传递
3. **注释说明**：每个配置项必须有注释说明
4. **默认值**：所有配置项必须提供合理的默认值

## 5. 表结构设计规范

### 5.1 Kafka源表规范

```sql
CREATE TABLE business_event_source (
    domain STRING COMMENT '业务域',
    type STRING COMMENT '事件类型', 
    timestamp BIGINT COMMENT '事件时间戳',
    eventId STRING COMMENT '事件ID',
    payload STRING COMMENT '载荷数据',
    proc_time AS PROCTIME() COMMENT '处理时间',
    event_time AS TO_TIMESTAMP_LTZ(timestamp, 3) COMMENT '事件时间',
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'business-events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json'
);
```

### 5.2 MySQL维表规范

```sql
CREATE TABLE user_dim (
    user_id STRING COMMENT '用户ID',
    user_name STRING COMMENT '用户名称',
    user_level INT COMMENT '用户等级',
    create_time TIMESTAMP(3) COMMENT '创建时间',
    update_time TIMESTAMP(3) COMMENT '更新时间',
    PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/dim_db',
    'table-name' = 'user_dim',
    'lookup.cache.max-rows' = '1000',
    'lookup.cache.ttl' = '60s'
);
```

### 5.3 MySQL结果表规范

```sql
CREATE TABLE user_wide_table (
    event_id STRING COMMENT '事件ID',
    user_id STRING COMMENT '用户ID', 
    user_name STRING COMMENT '用户名称',
    event_type STRING COMMENT '事件类型',
    business_data STRING COMMENT '业务数据',
    event_time TIMESTAMP(3) COMMENT '事件时间',
    process_time TIMESTAMP(3) COMMENT '处理时间',
    PRIMARY KEY (event_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/result_db',
    'table-name' = 'user_wide_table'
);
```

## 6. 测试规范

### 6.1 单元测试

```java
@Test
public void testUserEventProcessor() {
    UserEventProcessor processor = new UserEventProcessor();
    
    // 准备测试数据
    BusinessEvent event = createTestEvent();
    
    // 执行测试
    Object result = processor.process(event);
    
    // 验证结果
    assertNotNull(result);
    // 其他断言...
}
```

### 6.2 集成测试

```java
@Test
public void testUserDataStreamApp() {
    // 创建测试环境
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);
    
    // 创建测试数据源
    DataStreamSource<BusinessEvent> testSource = env.fromCollection(createTestEvents());
    
    // 执行业务逻辑
    // ...
    
    // 验证结果
    // ...
}
```

### 6.3 测试规范

1. **覆盖率**：单元测试覆盖率不低于80%
2. **测试数据**：使用真实的业务数据进行测试
3. **异常测试**：必须测试异常处理逻辑
4. **性能测试**：关键功能必须进行性能测试

## 7. 部署规范

### 7.1 Docker化部署

```dockerfile
FROM flink:1.17-java11

# 复制应用JAR
COPY target/flink-realtime-project-1.0.jar /opt/flink/usrlib/

# 复制配置文件
COPY src/main/resources/application.properties /opt/flink/conf/

# 设置环境变量
ENV FLINK_PROPERTIES="jobmanager.rpc.address: jobmanager"
```

### 7.2 提交参数规范

```bash
# 提交DataStream作业
flink run \
  --class com.flink.realtime.app.UserDataStreamApp \
  --parallelism 4 \
  --jobname "用户事件处理作业" \
  /path/to/flink-realtime-project-1.0.jar

# 提交SQL作业  
flink run \
  --class com.flink.realtime.app.UserSqlApp \
  --parallelism 2 \
  --jobname "用户事件SQL处理作业" \
  /path/to/flink-realtime-project-1.0.jar
```

## 8. 监控规范

### 8.1 关键指标

1. **业务指标**
   - 事件处理量（TPS）
   - 事件处理延迟
   - 处理成功率

2. **技术指标**
   - CPU使用率
   - 内存使用率
   - Checkpoint成功率

### 8.2 告警设置

```yaml
# 告警规则示例
alerts:
  - name: "事件处理延迟过高"
    condition: "avg_processing_latency > 5000ms"
    severity: "warning"
  
  - name: "事件处理失败率过高" 
    condition: "failure_rate > 5%"
    severity: "critical"
```

## 9. 性能优化规范

### 9.1 并行度设置

- **数据源并行度**：建议设置为Kafka分区数
- **计算算子并行度**：根据CPU核心数和数据量设置
- **输出算子并行度**：根据下游系统承载能力设置

### 9.2 内存配置

```yaml
# JobManager内存配置
jobmanager.memory.process.size: 1600m
jobmanager.memory.jvm-heap.size: 1024m

# TaskManager内存配置  
taskmanager.memory.process.size: 4096m
taskmanager.memory.managed.size: 1024m
```

### 9.3 Checkpoint配置

```properties
# Checkpoint间隔
execution.checkpointing.interval=5000

# Checkpoint超时时间
execution.checkpointing.timeout=60000

# 并发Checkpoint数量
execution.checkpointing.max-concurrent-checkpoints=1
```

## 10. 故障处理规范

### 10.1 重启策略

```java
// 固定延迟重启策略
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
    3, // 重启次数
    Time.of(10, TimeUnit.SECONDS) // 重启间隔
));
```

### 10.2 异常处理

1. **业务异常**：记录日志并跳过处理
2. **系统异常**：记录日志并触发重启
3. **数据异常**：记录异常数据并继续处理

## 11. 代码审查清单

### 11.1 必检项

- [ ] 代码符合命名规范
- [ ] 异常处理完整
- [ ] 日志记录充分
- [ ] 单元测试覆盖
- [ ] 配置项完整
- [ ] 资源正确释放

### 11.2 性能检查

- [ ] 避免在热点路径进行重IO操作
- [ ] 状态大小合理控制
- [ ] 并行度设置合理
- [ ] 内存使用优化

## 12. 版本管理

### 12.1 版本号规范

采用语义化版本号：`主版本号.次版本号.修订号`

- **主版本号**：不兼容的API修改
- **次版本号**：向下兼容的功能性新增
- **修订号**：向下兼容的问题修正

### 12.2 发布规范

1. **开发分支**：`feature/xxx`
2. **测试分支**：`develop`
3. **生产分支**：`master`
4. **标签规范**：`v1.0.0`
