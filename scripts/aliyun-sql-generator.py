#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¿é‡Œäº‘Flink SQLä½œä¸šç”Ÿæˆå™¨ï¼ˆä¿®å¤ç‰ˆï¼‰
æ ¹æ®é…ç½®æ–‡ä»¶ç”Ÿæˆå¯ç›´æ¥åœ¨é˜¿é‡Œäº‘Flink SQLå¹³å°è¿è¡Œçš„SQLæ–‡ä»¶

@author: yangfanlin
@date: 2025-01-17
"""

import json
import os
import sys
from datetime import datetime

def generate_aliyun_flink_sql(config):
    """ç”Ÿæˆé˜¿é‡Œäº‘Flink SQLä½œä¸š"""
    
    domain = config["domain"]
    job_name = config["job_name"]
    source_table = config["source_table"]
    dim_tables = config.get("dim_tables", [])
    result_table = config["result_table"]
    event_types = config.get("event_types", [])
    parallelism = config.get("parallelism", 2)
    
    # ç”ŸæˆSQLå†…å®¹
    sql_content = f"""-- {job_name}
-- é˜¿é‡Œäº‘Flink SQLä½œä¸šè„šæœ¬
-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- ä¸šåŠ¡åŸŸ: {domain}
-- 
-- ä½¿ç”¨è¯´æ˜ï¼š
-- 1. å°†æ­¤SQLå†…å®¹å¤åˆ¶åˆ°é˜¿é‡Œäº‘Flink SQLå¼€å‘å¹³å°
-- 2. æ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹é…ç½®å‚æ•°ï¼ˆç”¨${{}}åŒ…å›´çš„å˜é‡ï¼‰
-- 3. åˆ›å»ºä½œä¸šå¹¶æäº¤è¿è¡Œ

-- =============================================
-- 1. åˆ›å»ºKafkaæºè¡¨
-- =============================================
CREATE TABLE {source_table['name']} (
"""
    
    # æ·»åŠ æºè¡¨å­—æ®µ
    for i, field in enumerate(source_table['fields']):
        comment = f" COMMENT '{field['comment']}'" if field.get('comment') else ""
        comma = "," if i < len(source_table['fields']) - 1 else ""
        sql_content += f"    {field['name']:<20} {field['type']:<15}{comment}{comma}\n"
    
    # æ·»åŠ è®¡ç®—åˆ—å’Œæ°´å°
    sql_content += """    -- è®¡ç®—åˆ—ï¼šå¤„ç†æ—¶é—´
    proc_time AS PROCTIME(),
    -- è®¡ç®—åˆ—ï¼šäº‹ä»¶æ—¶é—´
    event_time AS TO_TIMESTAMP_LTZ(`timestamp`, 3),
    -- æ°´å°å®šä¹‰ï¼šå…è®¸5ç§’ä¹±åº
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = '${ kafka.input.topic }',
    'properties.bootstrap.servers' = '${ kafka.bootstrap.servers }',
    'properties.group.id' = '${ kafka.group.id }',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);

"""
    
    # æ·»åŠ ç»´è¡¨
    if dim_tables:
        sql_content += """-- =============================================
-- 2. åˆ›å»ºMySQLç»´è¡¨
-- =============================================
"""
        for dim_table in dim_tables:
            sql_content += f"-- {dim_table['name']} ç»´è¡¨\n"
            sql_content += f"CREATE TABLE {dim_table['name']} (\n"
            
            for i, field in enumerate(dim_table['fields']):
                comment = f" COMMENT '{field['comment']}'" if field.get('comment') else ""
                comma = "," if i < len(dim_table['fields']) - 1 else ""
                sql_content += f"    {field['name']:<20} {field['type']:<15}{comment}{comma}\n"
            
            sql_content += f"""    PRIMARY KEY ({dim_table['key_field']}) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = '${{ mysql.url }}',
    'username' = '${{ mysql.username }}',
    'password' = '${{ mysql.password }}',
    'table-name' = '{dim_table['name']}',
    -- ç»´è¡¨æŸ¥è¯¢ç¼“å­˜é…ç½®
    'lookup.cache.max-rows' = '1000',
    'lookup.cache.ttl' = '60s',
    'lookup.max-retries' = '3',
    'lookup.cache.caching-missing-key' = 'true'
);

"""
    
    # æ·»åŠ ç»“æœè¡¨
    sql_content += """-- =============================================
-- 3. åˆ›å»ºMySQLç»“æœè¡¨
-- =============================================
"""
    sql_content += f"CREATE TABLE {result_table['name']} (\n"
    
    for i, field in enumerate(result_table['fields']):
        comment = f" COMMENT '{field['comment']}'" if field.get('comment') else ""
        comma = "," if i < len(result_table['fields']) - 1 else ""
        sql_content += f"    {field['name']:<20} {field['type']:<15}{comment}{comma}\n"
    
    sql_content += f"""    PRIMARY KEY ({result_table['key_field']}) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = '${{ mysql.url }}',
    'username' = '${{ mysql.username }}',
    'password' = '${{ mysql.password }}',
    'table-name' = '{result_table['name']}',
    -- æ‰¹é‡å†™å…¥é…ç½®
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '1s',
    'sink.max-retries' = '3',
    'sink.parallelism' = '{parallelism}'
);

-- =============================================
-- 4. ä¸»è¦ä¸šåŠ¡å¤„ç†SQL
-- =============================================
INSERT INTO {result_table['name']}
SELECT 
    -- åŸºç¡€å­—æ®µ
    s.eventId as event_id,
    s.domain,
    s.type,
"""
    
    # æ·»åŠ SELECTå­—æ®µ
    for i, field in enumerate(result_table['select_fields']):
        comma = "," if i < len(result_table['select_fields']) - 1 else ""
        sql_content += f"    {field}{comma}\n"
    
    sql_content += f"FROM {source_table['name']} s \n"
    
    # æ·»åŠ ç»´è¡¨å…³è”
    if dim_tables:
        sql_content += "-- ç»´è¡¨å…³è”\n"
        for dim_table in dim_tables:
            sql_content += f"LEFT JOIN {dim_table['name']} FOR SYSTEM_TIME AS OF s.proc_time AS {dim_table['alias']}\n"
            sql_content += f"    ON JSON_VALUE(s.payload, '$.{dim_table['join_field']}') = {dim_table['alias']}.{dim_table['key_field']}\n"
    
    sql_content += "WHERE \n"
    sql_content += "    -- è¿‡æ»¤æ¡ä»¶ï¼šåªå¤„ç†æŒ‡å®šçš„äº‹ä»¶ç±»å‹\n"
    
    # æ·»åŠ äº‹ä»¶ç±»å‹è¿‡æ»¤
    event_type_list = "', '".join(event_types)
    sql_content += f"    s.type IN ('{event_type_list}')\n"
    sql_content += f"    -- è¿‡æ»¤æ¡ä»¶ï¼šåªå¤„ç†æŒ‡å®šçš„ä¸šåŠ¡åŸŸ\n"
    sql_content += f"    AND s.domain = '{domain}'\n"
    sql_content += f"    -- è¿‡æ»¤æ¡ä»¶ï¼šè¿‡æ»¤ç©ºäº‹ä»¶ID\n"
    sql_content += f"    AND s.eventId IS NOT NULL;\n\n"
    
    # æ·»åŠ é…ç½®è¯´æ˜
    sql_content += f"""-- =============================================
-- é˜¿é‡Œäº‘Flink SQLä½œä¸šé…ç½®è¯´æ˜
-- =============================================
/*
éƒ¨ç½²æ—¶éœ€è¦é…ç½®çš„å‚æ•°ï¼š

1. Kafkaé…ç½®ï¼š
   kafka.input.topic={domain}-events
   kafka.bootstrap.servers=your-kafka-instance.kafka.cn-hangzhou.aliyuncs.com:9092
   kafka.group.id={domain}-event-processor
   
   # å¦‚æœä½¿ç”¨SASLè®¤è¯ï¼ˆé˜¿é‡Œäº‘Kafkaå®ä¾‹æ¨èï¼‰ï¼š
   kafka.security.protocol=SASL_PLAINTEXT
   kafka.sasl.mechanism=PLAIN
   kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="your_username" password="your_password";

2. MySQLé…ç½®ï¼š
   mysql.url=jdbc:mysql://your-rds-instance.mysql.rds.aliyuncs.com:3306/{domain}_db?useSSL=true&serverTimezone=UTC
   mysql.username=your_mysql_username
   mysql.password=your_mysql_password

3. ä½œä¸šé…ç½®å»ºè®®ï¼š
   - å¹¶è¡Œåº¦: {parallelism}
   - Checkpointé—´éš”: 60s
   - çŠ¶æ€åç«¯: RocksDB
   - é‡å¯ç­–ç•¥: fixed-delay (3æ¬¡ï¼Œ10sé—´éš”)

4. é˜¿é‡Œäº‘ç‰¹æœ‰é…ç½®ï¼š
   # MiniBatchä¼˜åŒ–
   table.exec.mini-batch.enabled: true
   table.exec.mini-batch.allow-latency: 1s
   table.exec.mini-batch.size: 1000
   
   # çŠ¶æ€TTL
   table.exec.state.ttl: 3600000
   
   # Checkpointé…ç½®
   execution.checkpointing.interval: 60s
   execution.checkpointing.timeout: 10min
   execution.checkpointing.max-concurrent-checkpoints: 1

5. ç›‘æ§æŒ‡æ ‡å…³æ³¨ï¼š
   - äº‹ä»¶å¤„ç†TPS
   - ç«¯åˆ°ç«¯å»¶è¿Ÿ
   - å¤„ç†æˆåŠŸç‡
   - CheckpointæˆåŠŸç‡
   - ç»´è¡¨å‘½ä¸­ç‡

6. æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š
   - æ ¹æ®Kafkaåˆ†åŒºæ•°è°ƒæ•´å¹¶è¡Œåº¦
   - è°ƒæ•´ç»´è¡¨ç¼“å­˜å¤§å°å’ŒTTL
   - ç›‘æ§èƒŒå‹çŠ¶æ€
   - åˆç†è®¾ç½®èµ„æºé…ç½®ï¼ˆCUæ•°é‡ï¼‰
*/"""
    
    return sql_content

def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 aliyun-sql-generator.py <config.json>")
        print("é…ç½®æ–‡ä»¶ç¤ºä¾‹è¯·å‚è€ƒ: examples/user-job-config.json")
        sys.exit(1)
    
    config_file = sys.argv[1]
    if not os.path.exists(config_file):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        sys.exit(1)
    
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        domain = config["domain"]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"generated/aliyun-sql/{domain}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”ŸæˆSQLæ–‡ä»¶
        sql_content = generate_aliyun_flink_sql(config)
        sql_file = os.path.join(output_dir, f"{domain}-aliyun-flink-job.sql")
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        print("\n" + "="*60)
        print("ğŸ‰ é˜¿é‡Œäº‘Flink SQLä½œä¸šç”ŸæˆæˆåŠŸï¼")
        print("="*60)
        print(f"ğŸ“ SQLæ–‡ä»¶: {sql_file}")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. ç™»å½•é˜¿é‡Œäº‘å®æ—¶è®¡ç®—Flinkç‰ˆæ§åˆ¶å°")
        print("   https://realtime-compute.console.aliyun.com/")
        print("2. åˆ›å»ºæ–°çš„SQLä½œä¸š")
        print("3. å¤åˆ¶ç”Ÿæˆçš„SQLå†…å®¹åˆ°ç¼–è¾‘å™¨")
        print("4. åœ¨'å‚æ•°é…ç½®'ä¸­è®¾ç½®å¿…è¦çš„å‚æ•°")
        print("5. é€‰æ‹©åˆé€‚çš„èµ„æºé…ç½®ï¼ˆCUæ•°é‡ï¼‰")
        print("6. è°ƒè¯•è¿è¡Œå¹¶å¯åŠ¨ä½œä¸š")
        print("\nğŸ”— å‚è€ƒæ–‡æ¡£:")
        print("- é˜¿é‡Œäº‘Flinkå®˜æ–¹æ–‡æ¡£: https://help.aliyun.com/zh/flink/realtime-flink/")
        print("- SQLå¼€å‘æŒ‡å—: https://help.aliyun.com/zh/flink/developer-reference/")
        
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
