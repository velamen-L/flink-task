#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIç¼–ç¨‹è„šæ‰‹æ¶ - Flinkä½œä¸šç”Ÿæˆå™¨
åŸºäºé˜¿é‡Œäº‘Catalogå’ŒåŠ¨æ€è·¯ç”±æ¶æ„ï¼Œè‡ªåŠ¨ç”ŸæˆFlinkä½œä¸šä»£ç 

åŠŸèƒ½ç‰¹è‰²ï¼š
1. åŸºäºæ¨¡æ¿å¿«é€Ÿç”ŸæˆFlinkSQLå’ŒDataStreamä½œä¸š
2. æ”¯æŒåŠ¨æ€è·¯ç”±å’Œçƒ­éƒ¨ç½²
3. é›†æˆé˜¿é‡Œäº‘Catalogï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®è¡¨ç»“æ„
4. æ”¯æŒæ–°äº‹ä»¶ç±»å‹çš„åŠ¨æ€æ·»åŠ 

ä½œè€…ï¼šAIä»£ç ç”Ÿæˆå™¨
åˆ›å»ºæ—¶é—´ï¼š2024
"""

import json
import os
import sys
import argparse
from datetime import datetime
from pathlib import Path

class FlinkJobGenerator:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.templates_dir = self.project_root / "templates"
        self.output_dir = self.project_root / "generated"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
        
    def generate_job(self, config_file, job_type="both"):
        """
        ç”ŸæˆFlinkä½œä¸š
        
        Args:
            config_file: ä½œä¸šé…ç½®æ–‡ä»¶è·¯å¾„
            job_type: ä½œä¸šç±»å‹ (sql|datastream|both)
        """
        print(f"ğŸš€ AI Flinkä½œä¸šç”Ÿæˆå™¨å¯åŠ¨")
        print(f"é…ç½®æ–‡ä»¶: {config_file}")
        print(f"ä½œä¸šç±»å‹: {job_type}")
        print("-" * 50)
        
        # åŠ è½½é…ç½®
        config = self._load_config(config_file)
        
        # éªŒè¯é…ç½®
        self._validate_config(config)
        
        # ç”Ÿæˆä½œä¸š
        generated_files = []
        
        if job_type in ["sql", "both"]:
            sql_file = self._generate_sql_job(config)
            generated_files.append(sql_file)
            
        if job_type in ["datastream", "both"]:
            java_file = self._generate_datastream_job(config)
            generated_files.append(java_file)
            
        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        config_file = self._generate_config_file(config)
        generated_files.append(config_file)
        
        # ç”Ÿæˆå¯åŠ¨è„šæœ¬
        script_file = self._generate_startup_script(config)
        generated_files.append(script_file)
        
        print("\nâœ… ä½œä¸šç”Ÿæˆå®Œæˆï¼")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        for file in generated_files:
            print(f"  ğŸ“„ {file}")
            
        print(f"\nğŸ“š ä½¿ç”¨æ–¹æ³•:")
        print(f"  FlinkSQL: å°†SQLæ–‡ä»¶æäº¤åˆ°VVPå¹³å°")
        print(f"  DataStream: ./generated/start-{config['domain']}-job.sh")
        print(f"  Javaç±»å·²ç”Ÿæˆåˆ°: src/main/java/com/flink/realtime/business/{config['domain'].title()}WideTableApp.java")
        
        return generated_files
    
    def _load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def _validate_config(self, config):
        """éªŒè¯é…ç½®æ–‡ä»¶"""
        required_fields = ["domain", "job_name", "event_types", "source_table", "result_table"]
        for field in required_fields:
            if field not in config:
                raise Exception(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
    
    def _generate_sql_job(self, config):
        """ç”ŸæˆFlinkSQLä½œä¸š"""
        print("ğŸ”§ ç”ŸæˆFlinkSQLä½œä¸š...")
        
        domain = config["domain"]
        job_name = config["job_name"]
        event_types = config["event_types"]
        
        # SQLæ¨¡æ¿
        sql_template = f"""-- ================================================================
-- {job_name} FlinkSQL ä½œä¸š - AIç”Ÿæˆ
-- ä¸šåŠ¡åŸŸ: {domain}
-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- ================================================================

-- è®¾ç½®ä½œä¸šé…ç½®
SET 'execution.checkpointing.interval' = '10s';
SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';
SET 'table.exec.state.ttl' = '1d';

-- ================================================================
-- ä½¿ç”¨é˜¿é‡Œäº‘Catalogä¸­å·²é…ç½®çš„è¡¨
-- æºè¡¨: {config['source_table']['name']}
-- ç»“æœè¡¨: {config['result_table']['name']}
-- ================================================================

"""
        
        # ä¸ºæ¯ä¸ªäº‹ä»¶ç±»å‹ç”Ÿæˆå¤„ç†è§†å›¾
        for event_type in event_types:
            event_name = event_type["type"]
            view_name = f"{domain}_{event_name}_events"
            
            sql_template += f"""-- åˆ›å»º{event_type['description']}å¤„ç†è§†å›¾
CREATE TEMPORARY VIEW {view_name} AS
SELECT 
    eventId,
    domain,
    type,
    timestamp,
    TO_TIMESTAMP_LTZ(timestamp, 3) as event_time,
    PROCTIME() as proc_time,
"""
            
            # æ·»åŠ payloadå­—æ®µæå–
            for field in event_type["payload_fields"]:
                field_name = field["name"]
                sql_template += f"    JSON_VALUE(payload, '$.{field_name}') as {field_name.lower()},\n"
            
            sql_template += f"""FROM {config['source_table']['name']} 
WHERE domain = '{domain}' 
  AND type = '{event_name}';

"""
        
        # ä¸ºæ¯ä¸ªäº‹ä»¶ç±»å‹ç”ŸæˆINSERTè¯­å¥
        for event_type in event_types:
            event_name = event_type["type"]
            view_name = f"{domain}_{event_name}_events"
            
            sql_template += f"""-- å¤„ç†{event_type['description']}
INSERT INTO {config['result_table']['name']}
SELECT 
    CAST(HASH_CODE(COALESCE(eventId)) AS BIGINT) as id,
    eventId as event_id,
    type as event_type,
    userid as user_id,
    questionid as question_id,
    -- æ ¹æ®äº‹ä»¶ç±»å‹æ·»åŠ ç‰¹å®šé€»è¾‘
"""
            
            if "add" in event_name:
                sql_template += """    '' as fix_id,
    CAST(NULL AS TIMESTAMP(3)) as fix_time,
    0 as fix_result,
    'æœªè®¢æ­£' as fix_result_desc,
"""
            elif "fix" in event_name:
                sql_template += """    eventId as fix_id,
    TO_TIMESTAMP_LTZ(CAST(fixtime AS BIGINT), 3) as fix_time,
    CASE WHEN fixresult = 'correct' THEN 1 ELSE 0 END as fix_result,
    CASE WHEN fixresult = 'correct' THEN 'è®¢æ­£æ­£ç¡®' ELSE 'è®¢æ­£é”™è¯¯' END as fix_result_desc,
"""
            
            sql_template += f"""    proc_time as process_time
FROM {view_name} e;

"""
        
        # ä¿å­˜SQLæ–‡ä»¶
        sql_file = self.output_dir / f"{domain}_wide_table.sql"
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write(sql_template)
        
        return sql_file
    
    def _generate_datastream_job(self, config):
        """ç”ŸæˆDataStreamä½œä¸š"""
        print("ğŸ”§ ç”ŸæˆDataStreamä½œä¸š...")
        
        domain = config["domain"]
        class_name = f"{domain.title()}WideTableApp"
        
        # Javaç±»æ¨¡æ¿
        java_template = f"""package com.flink.realtime.business;

import com.flink.realtime.bean.BusinessEvent;
import com.flink.realtime.bean.ProcessedEvent;
import com.flink.realtime.config.RoutingConfig;
import com.flink.realtime.common.AliyunFlinkUtils;
import com.flink.realtime.function.BusinessEventDeserializationSchema;
import com.flink.realtime.function.DynamicRoutingProcessFunction;
import com.flink.realtime.function.OutputRoutingProcessFunction;
import com.flink.realtime.sink.AliyunMySQLSinkFunction;
import com.flink.realtime.source.AliyunKafkaSourceBuilder;
import com.flink.realtime.source.DynamicRoutingConfigSource;
import com.flink.realtime.util.ConfigUtils;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.OutputTag;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * {config['job_name']} - AIç”Ÿæˆ
 * ä¸šåŠ¡åŸŸ: {domain}
 * ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
 * 
 * æ”¯æŒçš„äº‹ä»¶ç±»å‹:
"""
        
        for event_type in config["event_types"]:
            java_template += f" * - {event_type['type']}: {event_type['description']}\n"
        
        java_template += f""" * 
 * @author AIä»£ç ç”Ÿæˆå™¨
 */
public class {class_name} {{
    
    private static final Logger logger = LoggerFactory.getLogger({class_name}.class);
    
    public static void main(String[] args) throws Exception {{
        
        String domain = "{domain}";
        logger.info("å¯åŠ¨{{}}å®½è¡¨ä½œä¸šï¼Œä¸šåŠ¡åŸŸ: {{}}", "{config['job_name']}", domain);
        
        // 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        StreamExecutionEnvironment env = AliyunFlinkUtils.getAliyunStreamExecutionEnvironment(
                ConfigUtils.getInt("flink.parallelism", 2));
        
        // 2. åˆ›å»ºäº‹ä»¶æº
        KafkaSource<BusinessEvent> eventSource = AliyunKafkaSourceBuilder.buildAliyunKafkaSource(
                ConfigUtils.getString("kafka.bootstrap.servers"),
                domain + "-events",
                domain + "-wide-table-group",
                ConfigUtils.getString("kafka.username", null),
                ConfigUtils.getString("kafka.password", null)
        );
        
        DataStreamSource<BusinessEvent> eventStream = env.fromSource(
                eventSource,
                WatermarkStrategy.<BusinessEvent>forMonotonousTimestamps()
                        .withTimestampAssigner((event, timestamp) -> event.getTimestamp()),
                domain + " Event Source"
        );
        
        // 3. åˆ›å»ºåŠ¨æ€è·¯ç”±é…ç½®æº
        DataStreamSource<RoutingConfig> configStream = env.addSource(
                new DynamicRoutingConfigSource(domain), "Routing Config Source");
        
        MapStateDescriptor<String, RoutingConfig> configDescriptor = 
                new MapStateDescriptor<>("routing-config", String.class, 
                        TypeInformation.of(RoutingConfig.class));
        BroadcastStream<RoutingConfig> configBroadcast = configStream.broadcast(configDescriptor);
        
        // 4. åŠ¨æ€è·¯ç”±å¤„ç†
        SingleOutputStreamOperator<ProcessedEvent> processedStream = eventStream
                .connect(configBroadcast)
                .process(new DynamicRoutingProcessFunction(configDescriptor))
                .name("Dynamic Event Processing")
                .uid("dynamic-event-processing-" + domain);
        
        // 5. è¾“å‡ºè·¯ç”±
        OutputTag<ProcessedEvent> alertTag = new OutputTag<ProcessedEvent>("alert", 
                TypeInformation.of(ProcessedEvent.class));
        OutputTag<ProcessedEvent> metricsTag = new OutputTag<ProcessedEvent>("metrics", 
                TypeInformation.of(ProcessedEvent.class));
        
        SingleOutputStreamOperator<ProcessedEvent> routedStream = processedStream
                .process(new OutputRoutingProcessFunction(alertTag, metricsTag, null))
                .name("Output Routing")
                .uid("output-routing-" + domain);
        
        // 6. è¾“å‡ºåˆ°å®½è¡¨
        routedStream.addSink(new AliyunMySQLSinkFunction<>(
                getInsertSQL(),
                getParameterSetter(),
                100
        )).name("{domain}å®½è¡¨è¾“å‡º");
        
        // 7. æ‰§è¡Œä½œä¸š
        env.execute("{config['job_name']} Dynamic Job");
    }}
    
    private static String getInsertSQL() {{
        return "INSERT INTO {config['result_table']['name']} " +
               "(id, event_id, event_type, user_id, question_id, process_time) " +
               "VALUES (?, ?, ?, ?, ?, ?)";
    }}
    
    private static AliyunMySQLSinkFunction.SqlParameterSetter<ProcessedEvent> getParameterSetter() {{
        return (ps, event) -> {{
            ps.setLong(1, event.getOriginalEvent().getEventId().hashCode());
            ps.setString(2, event.getOriginalEvent().getEventId());
            ps.setString(3, event.getOriginalEvent().getType());
            // æ ¹æ®å…·ä½“ä¸šåŠ¡æ·»åŠ å­—æ®µæ˜ å°„
            ps.setTimestamp(6, new java.sql.Timestamp(event.getProcessTime()));
        }};
    }}
}}
"""
        
        # ä¿å­˜Javaæ–‡ä»¶åˆ°businessç›®å½•
        business_dir = Path("src/main/java/com/flink/realtime/business")
        business_dir.mkdir(parents=True, exist_ok=True)
        java_file = business_dir / f"{class_name}.java"
        with open(java_file, 'w', encoding='utf-8') as f:
            f.write(java_template)
        
        return java_file
    
    def _generate_config_file(self, config):
        """ç”Ÿæˆä½œä¸šé…ç½®æ–‡ä»¶"""
        print("ğŸ”§ ç”Ÿæˆä½œä¸šé…ç½®æ–‡ä»¶...")
        
        # ç”Ÿæˆè¿è¡Œæ—¶é…ç½®
        runtime_config = {
            "domain": config["domain"],
            "job_name": config["job_name"],
            "kafka": {
                "bootstrap_servers": "localhost:9092",
                "group_id": f"{config['domain']}-wide-table-group"
            },
            "flink": {
                "parallelism": 2,
                "checkpoint_interval": "10s"
            },
            "generated_time": datetime.now().isoformat(),
            "supported_event_types": [et["type"] for et in config["event_types"]]
        }
        
        config_file = self.output_dir / f"{config['domain']}_job_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(runtime_config, f, indent=2, ensure_ascii=False)
        
        return config_file
    
    def _generate_startup_script(self, config):
        """ç”Ÿæˆå¯åŠ¨è„šæœ¬"""
        print("ğŸ”§ ç”Ÿæˆå¯åŠ¨è„šæœ¬...")
        
        domain = config["domain"]
        script_content = f"""#!/bin/bash

# {config['job_name']} å¯åŠ¨è„šæœ¬ - AIç”Ÿæˆ
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

set -e

DOMAIN="{domain}"
PROJECT_ROOT="$(cd "$(dirname "${{BASH_SOURCE[0]}}")/.." && pwd)"
JAR_FILE="$PROJECT_ROOT/target/flink-task-1.0-SNAPSHOT.jar"
MAIN_CLASS="com.flink.realtime.business.{domain.title()}WideTableApp"

echo "ğŸš€ å¯åŠ¨${{DOMAIN}}å®½è¡¨ä½œä¸š..."
echo "JARæ–‡ä»¶: $JAR_FILE"
echo "ä¸»ç±»: $MAIN_CLASS"

# æ£€æŸ¥JARæ–‡ä»¶
if [[ ! -f "$JAR_FILE" ]]; then
    echo "âŒ JARæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ç¼–è¯‘..."
    cd "$PROJECT_ROOT"
    mvn clean package -DskipTests
fi

# å¯åŠ¨ä½œä¸š
if [[ -n "$FLINK_HOME" ]]; then
    echo "âœ… æäº¤åˆ°Flinké›†ç¾¤..."
    $FLINK_HOME/bin/flink run \\
        --class "$MAIN_CLASS" \\
        --parallelism 2 \\
        "$JAR_FILE" \\
        "$DOMAIN"
else
    echo "âŒ è¯·è®¾ç½®FLINK_HOMEç¯å¢ƒå˜é‡"
    exit 1
fi
"""
        
        script_file = self.output_dir / f"start-{domain}-job.sh"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # æ·»åŠ æ‰§è¡Œæƒé™
        os.chmod(script_file, 0o755)
        
        return script_file

def main():
    parser = argparse.ArgumentParser(description='AI Flinkä½œä¸šç”Ÿæˆå™¨')
    parser.add_argument('config', help='ä½œä¸šé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--type', choices=['sql', 'datastream', 'both'], 
                       default='both', help='ç”Ÿæˆçš„ä½œä¸šç±»å‹')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    try:
        generator = FlinkJobGenerator()
        if args.output:
            generator.output_dir = Path(args.output)
            generator.output_dir.mkdir(exist_ok=True)
        
        generator.generate_job(args.config, args.type)
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
